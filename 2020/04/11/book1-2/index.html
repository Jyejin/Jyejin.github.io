<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="yanm1ng&#39;s blog">
  <meta name="keyword" content="hexo-theme, vuejs">
  
    <link rel="shortcut icon" type="image/ico" href="/gallery/berry.png"/>
  
  <title>
    
      CH3. 딥러닝의 시작2, 신경망 - 활성화 함수 | 개발계발 블로그
    
  </title>
  <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  
<link rel="stylesheet" href="/css/style.css">

  
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.js"></script>
  
  
  
  
    <!-- MathJax support START -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- MathJax support END -->
  


  
<meta name="generator" content="Hexo 4.2.0"></head>
<div class="wechat-share">
  <img src="/css/images/logo.png" />
</div>

  <body>
    <header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>개발계발 블로그</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
      </ul>
    </div>
  </div>
</header>

    <div id="article-banner">
  <h2>CH3. 딥러닝의 시작2, 신경망 - 활성화 함수</h2>
  <p class="post-date">2020-04-11</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><p>앞선 글에서 퍼셉트론, 가중치, 편향, 비선형, 다중퍼셉트론을 살펴보았다. 퍼셉트론 식을 구현하는 데 있어 가중치와 편향을 적절한 값으로 직접 설정했다. 그러나 층이 많아질수록 직접 설정할 수 없을 것이다. 신경망은 학습을 통해 가중치, 편향에 대한 적절한 값을 찾아준다.</p>
<h4 id="신경망"><a href="#신경망" class="headerlink" title="신경망"></a>신경망</h4> <img src="/gallery/book1-2-1.jpg" width="300px" />

<h4 id="신경망-네트워크"><a href="#신경망-네트워크" class="headerlink" title="신경망 네트워크"></a>신경망 네트워크</h4><p>신경망 네트워크는 입력층 - 은닉층 - 출력층으로 구성되는데, 은닉층의 경우 처리 과정을 확인할 수 없다.</p>
<p>신경망의 구조는 다층 퍼셉트론과 유사하다. 퍼셉트론에서 신경망으로 나아가 보자.</p>
<a id="more"></a> 


<p>앞서 정의한 퍼셉트론 식을 다시 보자.</p>
 <img src="/gallery/book1-2-2.jpg" width="600px" />


<h4 id="단층-퍼셉트론-식"><a href="#단층-퍼셉트론-식" class="headerlink" title="단층 퍼셉트론 식"></a>단층 퍼셉트론 식</h4><p>x1,x2는 입력신호, w1,w2는 가중치, b는 편향이다. 이 3가지를 네트워크로 나타내보자.</p>
 <img src="/gallery/book1-2-3.jpg" width="400px" />


<h4 id="단층-퍼셉트론-네트워크"><a href="#단층-퍼셉트론-네트워크" class="headerlink" title="단층 퍼셉트론 네트워크"></a>단층 퍼셉트론 네트워크</h4><p>익숙한 그림이지만 편향이 추가됐다. 이 그림은 위 퍼셉트론 식을 1<em>b + w1</em>x1 + w2*x2라고 풀어서 나타낸 것이다.</p>
<p>우리는 이 식을 새로 정의하여 아래처럼 간략화할 수 있다.</p>
 <img src="/gallery/book1-2-4.jpg" width="400px" />


<p>이 식을 설명하면 기존의 계산식(b+w1x1+w2x2)이 h(x) 함수를 거쳐 출력신호 y를 반환한다. 이 때, h(x)함수는 입력이 0을 넘으면 1을 반환하고 있다. h(x)함수는 활성화 함수라 부르며, 입력 신호의 총합을 처리하여 출력 값을 정하는 역할을 한다. </p>
<p>다시 말해, 기존의 단층 퍼셉트론 식은 입력신호의 총합을 갖고 출력신호를 반환했지만, 이제는 입력신호의 총합이 활성화 함수를 거쳐서 출력신호를 반환한다.</p>
<p>이렇게 활성화 함수를 추가하는 이유는 비선형성을 추가하기 위함이다. 바로 살펴보겠지만 활성화 함수들은 모두 비선형성 함수이다. 활성화 함수가 선형 함수라면 층을 깊게 하는 의미가 없어진다. 왜냐하면 복합함수로 설명될 수 있기 때문이다. f(x) * f(x) * f(x) 이런 식으로…</p>
<p>즉, 은닉층이 없는 네트워크가 된다. 그래서 다층 퍼셉트론에서는 활성화 함수가 필요하다.</p>
<p>활성화 함수를 네트워크 그림에 포함하면 아래와 같다.<br> <img src="/gallery/book1-2-5.jpg" width="400px" /></p>
<p>입력 신호의 총합(a)은 활성화함수(h())를 거쳐 출력값(y)를 반환한다.</p>
<h4 id="활성화-함수"><a href="#활성화-함수" class="headerlink" title="활성화 함수"></a>활성화 함수</h4><p>활성화 함수의 역할을 하는 함수가 여럿있는데 대표적으로는 계단함수, 시그모이드함수, ReLu함수가 있다. 이 함수가 어떻게 활성화시키는지 살펴보자.</p>
<h4 id="계단-함수"><a href="#계단-함수" class="headerlink" title="계단 함수"></a>계단 함수</h4><p> 계단 함수는 임계값 이전에는 출력값이 0이 었다가 임계값을 넘으면 1이 되는 함수이다. 계단 함수를 구현한 식은 아래와 같다. 여기서 임계값은 0이다.</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def step_function(x):</span><br><span class="line">    <span class="built_in">return</span> np.array(x&gt;0, dtype=np.int)</span><br></pre></td></tr></table></figure>

<p>구현한 식을 가지고 그래프를 그려보자.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pylab as plt</span><br><span class="line"></span><br><span class="line">x = np.arange(-5.0, 5.0, 0.1)</span><br><span class="line">y = step_function(x)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.ylim(-0.1, 1.1)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
 <img src="/gallery/book1-2-6.jpg" width="600px" />

<h4 id="계단-함수-그래프"><a href="#계단-함수-그래프" class="headerlink" title="계단 함수 그래프"></a>계단 함수 그래프</h4><p>그래프와 같이 x &lt;= 0 이면, y값은 계속 0이다가, x &gt; 0 이면 y값이 1이 된다. 앞서 언급한 단층 퍼셉트론이 이 경우에 해당한다.</p>
<h4 id="시그모이드-함수"><a href="#시그모이드-함수" class="headerlink" title="시그모이드 함수"></a>시그모이드 함수</h4><p> 시그모이드 함수는 신경망에서 자주 이용하는 활성화 함수로 수식은 다음과 같다. exp()는 지수함수를 의미한다.</p>
 <img src="/gallery/book1-2-7.jpg" width="300px" />

<p>시그모이드 함수는 아래와 같이 구현할 수 있다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def sigmoid(x):</span><br><span class="line">    <span class="built_in">return</span> 1/ (1+np.exp(-x))</span><br></pre></td></tr></table></figure>

<p>시그모이드 함수를 그래프로 나타내보자.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(-5.0, 5.0, 0.1)</span><br><span class="line">y = sigmoid(x)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.ylim(-0.1, 1.1)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
 <img src="/gallery/book1-2-8.jpg" width="600px" />

<p>계단함수처럼 이분적이지 않고 x값에 따라 y값이 계속 달라진다. </p>
<h4 id="ReLu-함수"><a href="#ReLu-함수" class="headerlink" title="ReLu 함수"></a>ReLu 함수</h4><p>ReLu 함수도 신경망에서 주로 이용하는 함수중 하나로, 입력이 0을 넘으면 입력을 그대로 출력하고 0이하이면 0을 출력한다.</p>
<p>수식과 그래프는 다음과 같다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def relu(x):</span><br><span class="line">    <span class="built_in">return</span> np.maximum(0, x)</span><br><span class="line">x = np.arange(-5.0, 5.0, 0.1)</span><br><span class="line">y = relu(x)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.ylim(-0.1, 1.1)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
 <img src="/gallery/book1-2-9.jpg" width="600px" />

<h4 id="출력층-활성화-함수"><a href="#출력층-활성화-함수" class="headerlink" title="출력층 활성화 함수"></a>출력층 활성화 함수</h4><p> 앞서 신경망은 입력층 - 은닉층 - 출력층으로 구성되어 있다고 언급했는데, 출력층에서의 활성화 함수는 다른 함수를 사용한다. 일반적으로신경망이 어떤 문제를 해결하느냐에 따라 다른 함수를 사용하는데, 분류 문제를 해결하는 경우 softmax함수를 사용하고 회귀에서는 항등함수를 사용한다. 항등 함수부터 살펴보자.</p>
<h4 id="항등-함수-identity-function"><a href="#항등-함수-identity-function" class="headerlink" title="항등 함수(identity function)"></a>항등 함수(identity function)</h4><p> 항등 함수는 간단하다. 입력 값을 그대로 출력한다. f(x) = x 이다. 그래서 출력층에서 항등 함수를 사용하면 입력 신호가 그대로 출력 신호가 된다. </p>
<h4 id="소프트맥스-함수-softmax-function"><a href="#소프트맥스-함수-softmax-function" class="headerlink" title="소프트맥스 함수(softmax function)"></a>소프트맥스 함수(softmax function)</h4><p> 소프트맥스 함수의 식은 다음과 같다.<br> <img src="/gallery/book1-2-10.jpg" width="300px" /></p>
<p>소프트맥수 함수의 분자는 입력 신호 ak의 지수함수, 분모는 모든 입력 신호의 지수 함수의 합으로 구성된다. 소프트맥스 함수를 그림으로 나타내면 다음과 같은데, 소프트맥스의 출력은 모든 입력 신호로부터 화살표를 받는다. 식의 분모에서 보듯, 출력층은 모든 입력 신호에서 영향을 받기 때문이다.</p>
 <img src="/gallery/book1-2-11.jpg" width="300px" /></section>
    <!-- Tags START -->
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#딥러닝" >
    <span class="tag-code">딥러닝</span>
  </a>

  <a href="/tags#study" >
    <span class="tag-code">study</span>
  </a>

      </div>
    
    <!-- Tags END -->
    <!-- NAV START -->
    
  <div class="nav-container">
    <!-- reverse left and right to put prev and next in a more logic postition -->
    
      <a class="nav-left" href="/2020/04/11/book1-1/">
        <span class="nav-arrow">← </span>
        
          CH2.딥러닝의 시작, 퍼셉트
        
      </a>
    
    
      <a class="nav-right" href="/2020/04/11/book1-3/">
        
          CH4. 신경망 학습하기-1 (손실함수, 교차엔트로피오차)
        
        <span class="nav-arrow"> →</span>
      </a>
    
  </div>

    <!-- NAV END -->
    <!-- 打赏 START -->
    
    <!-- 打赏 END -->
    <!-- 二维码 START -->
    
    <!-- 二维码 END -->
    
      <!-- No Comment -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
    <aside class="catalog-container">
  <div class="toc-main">
    <strong class="toc-title">Catalog</strong>
    
      <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#신경망"><span class="toc-nav-text">신경망</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#신경망-네트워크"><span class="toc-nav-text">신경망 네트워크</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#단층-퍼셉트론-식"><span class="toc-nav-text">단층 퍼셉트론 식</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#단층-퍼셉트론-네트워크"><span class="toc-nav-text">단층 퍼셉트론 네트워크</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#활성화-함수"><span class="toc-nav-text">활성화 함수</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#계단-함수"><span class="toc-nav-text">계단 함수</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#계단-함수-그래프"><span class="toc-nav-text">계단 함수 그래프</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#시그모이드-함수"><span class="toc-nav-text">시그모이드 함수</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#ReLu-함수"><span class="toc-nav-text">ReLu 함수</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#출력층-활성화-함수"><span class="toc-nav-text">출력층 활성화 함수</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#항등-함수-identity-function"><span class="toc-nav-text">항등 함수(identity function)</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#소프트맥스-함수-softmax-function"><span class="toc-nav-text">소프트맥스 함수(softmax function)</span></a></li></ol>
    
  </div>
</aside>
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'https://jyejin.github.io/2020/04/11/book1-2/';
    var banner = ''
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

    // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', 'http://file.muyutech.com/error-img.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== 'http://file.muyutech.com/error-img.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()

        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })
  })();
</script>







    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2020 | Proudly powered by <a href="https://hexo.io" target="_blank">Hexo</a>
    <br>
    Theme by <a href="https://github.com/yanm1ng" target="_blank" rel="noopener">yanm1ng</a>
  </p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->


<script src="/js/script.js"></script>

  </body>
</html>