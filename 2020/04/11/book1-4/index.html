<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="yanm1ng&#39;s blog">
  <meta name="keyword" content="hexo-theme, vuejs">
  
    <link rel="shortcut icon" type="image/ico" href="/gallery/berry.png"/>
  
  <title>
    
      CH5. 신경망 학습하기-2 (경사하강법) | 개발계발 블로그
    
  </title>
  <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  
<link rel="stylesheet" href="/css/style.css">

  
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.js"></script>
  
  
  
  
    <!-- MathJax support START -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- MathJax support END -->
  


  
<meta name="generator" content="Hexo 4.2.0"></head>
<div class="wechat-share">
  <img src="/css/images/logo.png" />
</div>

  <body>
    <header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>개발계발 블로그</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
      </ul>
    </div>
  </div>
</header>

    <div id="article-banner">
  <h2>CH5. 신경망 학습하기-2 (경사하강법)</h2>
  <p class="post-date">2020-04-11</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><p>지난 챕터에서는 데이터를 예측하고 예측 값에 대한 손실함수 구하기를 살펴보았다. </p>
<p>지난 챕터 바로가기 : <a href="https://myphiloprogramming.tistory.com/22" target="_blank" rel="noopener">https://myphiloprogramming.tistory.com/22</a></p>
<p>다음 순서를 계속 진행해보자.</p>
<ol start="4">
<li><p>경사하강법으로 가중치 값 개선하기</p>
<a id="more"></a> 


</li>
</ol>
<p>우리는 손실 함수 값을 줄여나감으로써 최적의 매개변수를 찾는다. 손실 함수 값을 줄이는 방법으로는 경사하강법을 사용하는데,</p>
<p>적용하기 전에 그 방법을 이해해보자.</p>
<p>먼저, 손실 함수값을 좌표 위에 찍어서 현재 위치를 확인한다. 적기로는 좌표 위에 점을 찍는다고 했지만, 실제로 차원은 가중치 매개변수 개수 만큼 있기 때문에 그릴수도, 그래프의 모양을 확인할 수도 없다.  적당히 이해하기로는 x축은 가중치 매개변수 개수 만큼있고 y축은 손실함수 값이 된다. 그림으로 나타내보면 다음과 같다.(이 그림은 이해를 돕기 위할 뿐이며 실제로는 그림으로 나타낼 수도 없다.)</p>
 <img src="/gallery/book1-4-1.jpg" width="400px" />


<p>전체 그래프를 모르기 때문에 어디가 손실함수 값의 최솟값인지 짐작할 수 없다. 이런 상황에서 기울기를 이용해 최솟값을 찾으려는 것이 경사법이다. 기울어진 방향에 꼭 최솟값이 있는 것은 아니지만 그 방향으로 갔을 때 손실함수 값을 줄일 수 있다. 그래서 기울기를 단서로 나아갈 방향을 정하게 된다. 기울기는 아래 그림처럼 방향을 가진 벡터로 그려진다.</p>
 <img src="/gallery/book1-4-2.jpg" width="400px" />


<p>기울기<br>화살표를 보면 한 곳을 향하고 있는데, 이 때 가리키는 위치가 가장 최솟값이 된다. 정리하면 현재 위치에서 기울기를 구한 후, 손실함수 값이 낮아지는 방향으로 이동한다.</p>
<p>경사하강법은 현 위치에서 기울어진 방향으로 일정 거리만큼이동한다. 그런 다음 이동한 곳에서 기울기를 한번 더 구하고 또 기울어진 방향으로 나아가기를 반복함으로써 최솟값을 찾아나간다. </p>
<p>경사하강법을 구현하면 다음과 같다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def numerical_gradient(f, x):</span><br><span class="line">    h = 1e-4</span><br><span class="line">    grad = np.zeros_like(x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(x.size):</span><br><span class="line">        tmp_val = x[idx]</span><br><span class="line">        x[idx] = tmp_val + h</span><br><span class="line">        fxh1 = f(x)</span><br><span class="line">        </span><br><span class="line">        x[idx] = tmp_val - h</span><br><span class="line">        fxh2 = f(x)</span><br><span class="line">        </span><br><span class="line">        grad[idx] = (fxh1 - fxh2) / (2*h)</span><br><span class="line">        x[idx] = tmp_val</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">return</span> grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def gradient_descent(f, init_x, lr=0.01, step_num=100):</span><br><span class="line">    x = init_x</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(step_num):</span><br><span class="line">        grad = numerical_gradient(f,x)</span><br><span class="line">        x -= lr * grad</span><br><span class="line">    <span class="built_in">return</span> x</span><br></pre></td></tr></table></figure>

<p>gradient_descent함수의 파라미터를 살펴보면, f는 최적화하려는 함수, init_x는 초깃값, lr은 학습률, step_num은 반복 횟수를 의미한다.</p>
<p>그리고 numerical_gradient 함수는 기울기를 구한다.</p>
<p>init_x = np.array([-3.0, 4.0])<br>gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)</p>
<p>gradient_descent 함수 실행 결과<br>gradient_descent함수를 사용하면 step_num만큼 반복하면서 찾은 최솟값의 위치를 리턴한다. 다시 말해, 손실함수가 최솟값이 되는 매개변수 값을 리턴한다. 위 예시에서는 초기 매개변수로 [-3.0, 4.0]을 넣었더니, [-6.11e-10, 8.14e-10] 결과가 반환됐다.</p>
<ol start="5">
<li>2,3,4 반복하며 최적값 찾기</li>
</ol>
<p>gradient_descent 함수가 리턴한 값은 개선된 가중치 값이다. 이제 이 값을 가지고 다시 숫자 이미지를 맞춘다. 다시 배치 데이터를 뽑고 새로 갱신된 가중치 값으로 예측한 후, 새로운 손실함수 값을 또 최소화도록 경사하강법을 적용한다. 이렇게 이 과정을 반복하면서 가중치 최적값을 찾아나간다.</p>
<p>1번부터 5번까지에 대한 전체 코드는 다음과 같다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">class TwoLayerNet:</span><br><span class="line">    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):</span><br><span class="line">        <span class="comment">#가중치 초기화</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">'W1'</span>] = weight_init_std * \</span><br><span class="line">                            np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">'W2'</span>] = weight_init_std * \</span><br><span class="line">                            np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">'b2'</span>] = np.zeros(output_size)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    def predict(self, x):</span><br><span class="line">        W1, W2 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'W2'</span>]</span><br><span class="line">        b1, b2 = self.params[<span class="string">'b1'</span>], self.params[<span class="string">'b2'</span>]</span><br><span class="line">        </span><br><span class="line">        a1 = np.dot(x, W1) + b1</span><br><span class="line">        z1 = sigmoid(a1)</span><br><span class="line">        a2 = np.dot(z1, W2) + b2</span><br><span class="line">        y = softmax(a2)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">return</span> y</span><br><span class="line">    </span><br><span class="line">    def loss(self, x, t):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">return</span> cross_entropy_error(y, t)</span><br><span class="line">    </span><br><span class="line">    def accuracy(self, x, t):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        y = np.argmax(y, axis=1)</span><br><span class="line">        t = np.argmax(t, axis=1)</span><br><span class="line">        </span><br><span class="line">        accuracy = np.sum(y==t) / <span class="built_in">float</span>(x.shape[0])</span><br><span class="line">        <span class="built_in">return</span> accuracy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def numerical_gradient(self, x, t):</span><br><span class="line">        loss_W = lambda W: self.loss(x,t)</span><br><span class="line">        </span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">'W1'</span>] = numerical_gradient(loss_W, self.params[<span class="string">'W1'</span>])</span><br><span class="line">        grads[<span class="string">'b1'</span>] = numerical_gradient(loss_W, self.params[<span class="string">'b1'</span>])    </span><br><span class="line">        grads[<span class="string">'W2'</span>] = numerical_gradient(loss_W, self.params[<span class="string">'W2'</span>])</span><br><span class="line">        grads[<span class="string">'b2'</span>] = numerical_gradient(loss_W, self.params[<span class="string">'b2'</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">return</span> grads</span><br><span class="line"><span class="comment">#미니 배치</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = \</span><br><span class="line">    load_mnist(normalize=True, one_hot_label=True)</span><br><span class="line"></span><br><span class="line">train_loss_list = []</span><br><span class="line"></span><br><span class="line">iters_num = 10000</span><br><span class="line">train_size = x_train.shape[0]</span><br><span class="line">batch_size = 100</span><br><span class="line">learning_rate = 0.1</span><br><span class="line"></span><br><span class="line">network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters_num):</span><br><span class="line">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[batch_mask]</span><br><span class="line">    </span><br><span class="line">    grad = network.numerical_gradient(x_batch, t_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">'W1'</span>,<span class="string">'b1'</span>,<span class="string">'W2'</span>,<span class="string">'b2'</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line">        </span><br><span class="line">    loss = network.loss(x_batch, t_batch)</span><br><span class="line">    train_loss_list.append(loss)</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">6. 테스트 데이터로 성능 테스트 해보기</span><br><span class="line"></span><br><span class="line">이로써 신경망 구현이 끝났다. 이제 테스트 데이터를 적용해서 정확도를 확인해보자.</span><br><span class="line"></span><br><span class="line">테스트 코드는 다음과 같다.</span><br><span class="line">``` bash</span><br><span class="line">(x_train, t_train), (x_test, t_test) = \</span><br><span class="line">    load_mnist(normalize=True, one_hot_label=True)</span><br><span class="line"></span><br><span class="line">network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)</span><br><span class="line"></span><br><span class="line">iters_num = 10000</span><br><span class="line">train_size = x_train.shape[0]</span><br><span class="line">batch_size = 100</span><br><span class="line">learning_rate = 0.1</span><br><span class="line"></span><br><span class="line">train_loss_list = []</span><br><span class="line">train_acc_list = []</span><br><span class="line">test_acc_list = []</span><br><span class="line"></span><br><span class="line">iters_per_epoch = max(train_size / batch_size, 1)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iters_num):</span><br><span class="line">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[bathc_mask]</span><br><span class="line">    </span><br><span class="line">    grad = network.numerical_gradient(x_bathc, t_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">'W1'</span>,<span class="string">'b1'</span>,<span class="string">'W2'</span>,<span class="string">'b2'</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line">        </span><br><span class="line">    loss = network.loss(x_batch, t_batch)</span><br><span class="line">    train_loss_list.append(loss)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> i % iter_per_epoch == 0:</span><br><span class="line">        train_acc = network.accuracy(x_train, t_train)</span><br><span class="line">        test_acc = network.accuracy(x_test, t_test)</span><br><span class="line">        train_acc_list.append(train_acc)</span><br><span class="line">        test_acc_list.append(test_acc)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"train acc, test acc |"</span> + str(train_acc) + <span class="string">", "</span> + str(test_acc))</span><br></pre></td></tr></table></figure></section>
    <!-- Tags START -->
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#딥러닝" >
    <span class="tag-code">딥러닝</span>
  </a>

  <a href="/tags#study" >
    <span class="tag-code">study</span>
  </a>

      </div>
    
    <!-- Tags END -->
    <!-- NAV START -->
    
  <div class="nav-container">
    <!-- reverse left and right to put prev and next in a more logic postition -->
    
      <a class="nav-left" href="/2020/04/11/book1-3/">
        <span class="nav-arrow">← </span>
        
          CH4. 신경망 학습하기-1 (손실함수, 교차엔트로피오차)
        
      </a>
    
    
      <a class="nav-right" href="/2020/04/12/book2-1/">
        
          JavaScript 클로저
        
        <span class="nav-arrow"> →</span>
      </a>
    
  </div>

    <!-- NAV END -->
    <!-- 打赏 START -->
    
    <!-- 打赏 END -->
    <!-- 二维码 START -->
    
    <!-- 二维码 END -->
    
      <!-- No Comment -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
    <aside class="catalog-container">
  <div class="toc-main">
    <strong class="toc-title">Catalog</strong>
    
      <ol class="nav">none</ol>
    
  </div>
</aside>
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'https://jyejin.github.io/2020/04/11/book1-4/';
    var banner = ''
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

    // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', 'http://file.muyutech.com/error-img.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== 'http://file.muyutech.com/error-img.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()

        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })
  })();
</script>







    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2020 | Proudly powered by <a href="https://hexo.io" target="_blank">Hexo</a>
    <br>
    Theme by <a href="https://github.com/yanm1ng" target="_blank" rel="noopener">yanm1ng</a>
  </p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->


<script src="/js/script.js"></script>

  </body>
</html>